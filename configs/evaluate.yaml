name: Evaluate
description: Evaluates the trained keras model, saves the evaludation metrics to the
  metadata store
inputs:
- {name: feature, type: String, description: Feature strin to train on}
- {name: lookback, type: Integer, description: Length of the lookback window}
- {name: batch_size, type: Integer, description: Batch size}
- {name: test_data, type: Dataset, description: Train dataset}
- {name: scaler_model, type: Model, description: Scaler model}
- {name: keras_model, type: Model, description: Keras model}
outputs:
- {name: metrics, type: Metrics}
implementation:
  container:
    image: tensorflow/tensorflow:latest-gpu
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'scikit-learn' 'google-cloud-aiplatform' 'protobuf==3.13.0' 'kfp==1.8.14' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef evaluate(\n    feature: str,\n    lookback: int,\n    batch_size:\
      \ int,\n    test_data: Input[Dataset],\n    scaler_model: Input[Model],\n  \
      \  keras_model: Input[Model],\n    metrics: Output[Metrics],\n) -> None:\n \
      \   \"\"\"Evaluates the trained keras model, saves the evaludation metrics to\
      \ the metadata store\n\n    Args:\n        feature (str): Feature strin to train\
      \ on\n        lookback (int): Length of the lookback window\n        batch_size\
      \ (int): Batch size\n        test_data (Input[Dataset]): Train dataset\n   \
      \     scaler_model (Input[Model]): Scaler model\n        keras_model (Input[Model]):\
      \ Keras model\n        eval_metrics (Output[Metrics]): Metrics\n    \"\"\"\n\
      \    import os\n    import json\n    from datetime import datetime\n\n    import\
      \ google.cloud.aiplatform as aip\n\n    import joblib\n    import numpy as np\n\
      \    import pandas as pd\n    from tensorflow import keras\n\n    PROJECT_ID\
      \ = 'test-rig-349313'\n    REGION = 'europe-west2'\n    EXP_NAME = feature.lower().replace('_',\
      \ '-')\n    TIMESTAMP = datetime.now().strftime('%Y%m%d%H%M%S')\n\n    aip.init(\n\
      \        experiment=EXP_NAME,\n        project=PROJECT_ID,\n        location=REGION,\n\
      \    )\n    aip.start_run(run='-'.join((EXP_NAME, TIMESTAMP)))\n    test_df\
      \ = pd.read_csv(test_data.path + '.csv', index_col=False)\n    test_data = test_df[feature].values.reshape(-1,\
      \ 1)\n    scaler = joblib.load(scaler_model.path + '.joblib')\n    scaled_test\
      \ = scaler.transform(test_data)\n    x_test, y_test = [], []\n    for i in range(lookback,\
      \ len(scaled_test)):\n        x_test.append(scaled_test[i - lookback:i])\n \
      \       y_test.append(scaled_test[i])\n    x_test = np.stack(x_test)\n    y_test\
      \ = np.stack(y_test)\n    forecaster = keras.models.load_model(keras_model.path\
      \ + '.h5')\n    results = forecaster.evaluate(x_test,\n                    \
      \              y_test,\n                                  verbose=1,\n     \
      \                             batch_size=batch_size,\n                     \
      \             return_dict=True)\n    with open(metrics.path + '.json', 'w')\
      \ as metrics_file:\n        metrics_file.write(json.dumps(results))\n    with\
      \ open(os.path.join('gcs', 'models_forecasting', f'{feature}.json'),\n     \
      \         'w') as metrics_file:\n        metrics_file.write(json.dumps(results))\n\
      \    for k, v in results.items():\n        metrics.log_metric(k, v)\n      \
      \  aip.log_metrics({k: v})        \n    metrics.metadata['feature'] = feature\n\
      \    aip.end_run()\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - evaluate
